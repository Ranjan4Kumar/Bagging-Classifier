{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Ensmeble methods\n",
        "This is a machine learning technique that combines several models in order to produce one optimal predictive model\n",
        "The goal of ensemble methods to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability/robutness over single estimator.\n",
        "\n",
        "### Types of Ensemble Learning:\n",
        "There are 2 popular ensemble technique\n",
        "1. Bagging (parallel Technique)\n",
        "2. Boosting (Sequential Ensemble Technique)\n",
        "\n",
        "\n",
        "In our train_test_split we have term called random_state\n",
        " - which enseure that the split is consistent across diffewrent runs of your code.\n",
        " - You can shuffle your data to avoid any ordering biases. You can use 'random_state' to control the shuffling.\n",
        " - Random number generator - if you need to genrate random numbers for any part of your code using 'random_state' ensure that the same sequence of the random numbers is generated each time making your result rpeproducible.\n",
        "\n",
        "\n",
        "\n",
        "### Bagging\n",
        "Bagging(or Bootstrap aggregatng) is a type of ensemble learning in which multiple base models are trained independently in parallel on different subset of the training data.(random_state = 40).\n",
        "Each subset is generated using bootstrap sampling , in which data points are picked at random with replacement.\n",
        "\n",
        "In case of the bagging classifier, the final prediction is make by aggreagting the predictions of the all-base model, isng majority voting.\n",
        "\n",
        "\n",
        "In case of regression, the final prediction is made by averaging the predictions of all-base model, and that is known as bagging regression.\n",
        "\n",
        "#### How does bagging Clasifier works\n",
        "\n",
        " <b>. Step -1</b>\n",
        "- Boostrap Sampling : 'n' subset origina; training data are sampled with replacement. This step ensure that th base model are trained on diverse subset of data, as some samples may appear multiple times in the new subset,while other may be omitted. It reduces the risk of overfitting and improvs the accuracy of the model.\n",
        "\n",
        "original training dataset : [1,2,3,4,5,6,7,8,9,10]\n",
        "Resamples training set 1:   [2,3,3,5,6,1,8,10,9,1]\n",
        "Resamples training set 2:   [1,1,5,6,3,8,9,10,2,7]\n",
        "Resample training set 3:    [5,1,3,2,5,8,8,7,2,6,10]\n",
        "\n",
        "<b> step -2 </b>\n",
        "- Base model training : In bagging , multiple base models are used. After the Bootstrap sampling, each base model is independently trained using specific learning algorithm, such as deicision tree, support vector machine or neural networks on a different bootstrap subset of data. These models are typically called \"weak learners\" because they may not be highly accurate on their own.\n",
        "Scince the base model is trianed independenty of different subsets of data. To mmake the model comoutationally efficient and lless time-consumin, the base models can be trained in parallel.\n",
        "\n",
        "<b> step -3 </b>\n",
        "Aggregation : Once all the base models are trained , it is used to make predictions on the unseen data. the subset of data on whihc that base model is not traind. In the bagging classifier , the predicted class label for the given instnace is choosen based in the majority Voting. The calss which has the majoiryt voting is the prediction of the model.\n",
        "\n",
        "\n",
        "### Advantages of bagging Classifier:\n",
        "- 1. Improved Predictive Performance : Bagging Clasifier often outperform single cllassifier by reducing overfitting and increaeing the predictive accuracy. By combning multile base models, it can better generalize to unseen data.\n",
        "\n",
        "- 2. Reduce Variance : Since each base model is. trained on diferent subsets of the data, the aggregated models variance is significantly reduce compared to an individula model.\n",
        "\n",
        "- 3. Parallelization - Bagging allows for parallel processing , as each base model can be trained independently. This makes it computationally efficient , especially for large datasets.\n",
        "\n",
        "- 5. Flexibility : this technique that can be applied to wide range of machine learning algorithm , inculding deicion trees, random forest and support vector machines.\n",
        "\n",
        "### Application\n",
        "1. Fraud dtetction\n",
        "2 . Spam filtering\n",
        "3. Credit scoring\n",
        "4. Image classification\n",
        "5. Natural language processing.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cvqR4wtE6dFb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GWB0rSsBwtI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4yaxWArf6FM4"
      },
      "outputs": [],
      "source": [
        "# import the necessay libraires\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "time1 = time.time()\n",
        "\n",
        "digit = load_digits()\n",
        "X, y = digit.data, digit.target\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train,y_test = train_test_split(X,y, test_size= 0.2,random_state = 42)\n",
        "# create the base classifier\n",
        "\n",
        "dc = [DecisionTreeClassifier(), SVC()]\n",
        "for i in range(len(dc)):\n",
        "  model  = BaggingClassifier(base_estimator = dc[i], n_estimators=10)\n",
        "  classifiers = model.fit(X_train,y_train)\n",
        "\n",
        "  # Make prediction on the test set\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # calculate accuraacy\n",
        "  accuracy = accuracy_score(y_test,y_pred)\n",
        "  print(\"Accuracy:\",accuracy)\n",
        "  i = i+1\n",
        "\n",
        "time2 = time.time()\n",
        "print(time2 -  time1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHx-leVdvQ4l",
        "outputId": "71347130-b1ea-43a9-e3a9-289602ca301e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9833333333333333\n",
            "1.122523307800293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i , clf in enumerate(classifiers):\n",
        "  y_pred = clf.predict(X_test)\n",
        "  # Calculate accuracy\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "  print(\"accuracy:\"+str(i+1), ':', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVUoLCDBvnW4",
        "outputId": "17ff3c81-c710-45ff-fc69-2562a759a66c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:1 : 0.8472222222222222\n",
            "accuracy:2 : 0.8472222222222222\n",
            "accuracy:3 : 0.8277777777777777\n",
            "accuracy:4 : 0.85\n",
            "accuracy:5 : 0.825\n",
            "accuracy:6 : 0.8361111111111111\n",
            "accuracy:7 : 0.85\n",
            "accuracy:8 : 0.8416666666666667\n",
            "accuracy:9 : 0.8472222222222222\n",
            "accuracy:10 : 0.8472222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "seR2kEvWvpBq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}